{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c72d0eb-c1db-47bc-8348-2af54a65c35c",
   "metadata": {},
   "source": [
    "Motivation. We may want our model to **combine** knowledge from different behaviors of the same attention mechanism\n",
    "- capturing dependencies of various ranges (e.g., shorter-range vs. longer-range) within a sequence. <br>\n",
    "Thus, it may be beneficial to allow our attention mechanism to jointly use different representation subspaces of queries, keys, and values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198bd553-eef2-4e05-9b7a-d1d5b9849444",
   "metadata": {},
   "source": [
    "- Instead of performing a single attention pooling, {queries, keys, and values} can be transformed with $h$ independently learned linear projections. <br> - $h$ attention pooling outputs are concatenated and transformed with another learned <font color='red'>linear projection</font> to produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700588c6-396d-45de-a2ea-8696ef31b98d",
   "metadata": {},
   "source": [
    "This design is called **multi-head attention**, where each of the $h$ attention\n",
    "pooling outputs is a **head**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b61b1-b319-421d-b84c-ce5aff24ef2f",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29740965-e252-46c5-a9b5-945fea2a7124",
   "metadata": {},
   "source": [
    "Given a query $\\mathbf{q} \\in \\mathbb{R}^{d_q}$, a key $\\mathbf{k} \\in \\mathbb{R}^{d_k}$, and a value $\\mathbf{v} \\in \\mathbb{R}^{d_v}$, each attention head $\\mathbf{h}_i (i=1,...,h)$ is computed as\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    \\mathbf{h}_i &= f(\\mathbf{W}_i^{(q)}\\mathbf{q}, \\mathbf{W}_i^{(k)}\\mathbf{k}, \\mathbf{W}_i^{(v)}\\mathbf{v}) \\in \\mathbb{R}^{p_v} \\\\\n",
    "                 &= \\sum_{j=1}^{m}{\\alpha{(\\mathbf{W}_i^{(q)}\\mathbf{q},\\mathbf{W}_i^{(k)}\\mathbf{k}_j}) \\mathbf{W}_i^{(v)}\\mathbf{v}_j} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where learnable parameters $\\mathbf{W}_i^{(q)} \\in \\mathbb{R}^{p_q \\times d_q}$, $\\mathbf{W}_i^{(k)} \\in \\mathbb{R}^{p_k \\times d_k}$ and $\\mathbf{W}_i^{(v)} \\in \\mathbb{R}^{p_v \\times d_v}$, and $f$ is <font color='red'>attention pooling, such as additive attention and scaled dot-product attention</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd539a0b-8c88-4c0b-8f9e-375798d430b8",
   "metadata": {},
   "source": [
    "The multi-head attention output is another linear transformation via learnable parameters $\\mathbf{W}_o \\in \\mathbb{R}^{p_o \\times hp_v}$ of the concatenation of $h$ heads:\n",
    "\\begin{equation}\n",
    "    \\mathbf{W}_o \\begin{bmatrix} \\mathbf{h}_1 \\\\ \\mathbf{h}_2 \\\\ \\vdots \\\\ \\mathbf{h}_h \\\\ \\end{bmatrix} \\in \\mathbb{R}^{p_o}.\n",
    "\\end{equation}\n",
    "Based on this design, <font color='red'>each head</font> may attend to <font color='red'>different parts of the input</font>. More sophisticated \n",
    "functions than the simple weighted average can be expressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f261100-2b89-44a1-8c26-5f402c65c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06069f8e-ecfc-44ba-bb53-e018029b0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(d2l.Module):\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of queries, keys, or values:\n",
    "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "        # After transposing, shape of output queries, keys, or values:\n",
    "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
    "        # num_hiddens / num_heads)\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "        if valid_lens is not None:\n",
    "            # On axis 0, copy the first item (scalar or vector) for num_heads\n",
    "            # times, then copy the next item, and so on\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries,\n",
    "        # num_hiddens / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "        # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "        # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "        # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "        # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f6df7-ec6f-4c09-8a14-96a19a7097e5",
   "metadata": {},
   "source": [
    "**REMARK**. To avoid significant growth of computational cost and parameterization cost, we set $p_q = p_k = p_v = p_o/h$. Above, $p_o$ is specified via ```num_hiddens```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eabc810-6b4c-42ff-9076-f8e2fee6b681",
   "metadata": {},
   "source": [
    "### nn.LazyLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84ede15-ed42-4ee0-bcf0-33d96aaa13e1",
   "metadata": {},
   "source": [
    "The framework defers initialization, waiting **until the first time we pass data** through the model, to infer the sizes of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85019a70-c653-47af-a440-7cc42ad34f11",
   "metadata": {},
   "source": [
    "**Parameters** <br>\n",
    "- out_features (int) – size of each output sample. <br>\n",
    "**Variables** <br>\n",
    "- weight (torch.nn.parameter.UninitializedParameter) – the learnable weights of the module of shape ( out_features , in_features ) (out_features,in_features). <br>\n",
    "- bias (torch.nn.parameter.UninitializedParameter) – the learnable bias of the module of shape ( out_features ) (out_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc562d2f-8ec6-40c3-82b7-a39bf33a0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ff06ea-acf2-4f1a-bcac-9bed57a0076d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<UninitializedParameter>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9edd40d-2a8a-4095-a731-e53ce40e8a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "net(X)\n",
    "net[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32f3472-2189-4d6e-83f8-b953b0434845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7cd66c1-440d-4756-8243-a733456ef0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 20])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 5, 20)\n",
    "net(X)\n",
    "net[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5542d-4b01-4f49-bb44-4aecc1606d0a",
   "metadata": {},
   "source": [
    "### torch.repeat_interleave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec054a0f-0133-47f8-981f-2cd40e456f07",
   "metadata": {},
   "source": [
    "```valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)``` <br>\n",
    "dim (int, optional) – The dimension along which to repeat values. By default, use the **flattened** input array, and return a flat output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb8898a-d3d4-4486-8452-4b5653b0ccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 2],\n",
       "        [3, 4],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "torch.repeat_interleave(x, repeats=2, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ac850-d811-44fc-8970-3be2466bb158",
   "metadata": {},
   "source": [
    "### transpose_qkv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df1c50-6c6e-4c7e-957f-1a20d8f11a4d",
   "metadata": {},
   "source": [
    "```queries = self.transpose_qkv(self.W_q(queries))```\n",
    "- Shape of queries, keys, or values: (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "- After transposing, shape of output queries, keys, or values: (batch_size * num_heads, no. of queries or key-value pairs, num_hiddens / num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d5c6a-4595-47f8-8c10-ee3ccd464a1e",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ccf85ba-284c-443f-8efb-ee9a81256269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\billt\\PycharmProjects\\DeepLearning\\dl_venv\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)\n",
    "batch_size, num_queries, num_kvpairs = 2, 4, 6\n",
    "valid_lens = torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "d2l.check_shape(attention(X, Y, Y, valid_lens),\n",
    "                (batch_size, num_queries, num_hiddens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad4345-2c81-4033-8c79-6fd6810c4947",
   "metadata": {},
   "source": [
    "### Variable dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c3472-3246-4ae6-9639-afcfd3a7ce16",
   "metadata": {},
   "source": [
    "queries.shape: torch.Size([2, 4, 100]) <br>\n",
    "keys.shape: torch.Size([2, 6, 100]) <br> \n",
    "self.W_k(keys).shape: torch.Size([2, 6, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef87594-835f-4b59-8ea3-2416ccea029d",
   "metadata": {},
   "source": [
    "**After** we pass data <br>\n",
    "queries.shape: torch.Size([10, 4, 20]) <br> \n",
    "keys.shape: torch.Size([10, 6, 20]) <br>\n",
    "values.shape: torch.Size([10, 6, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677d763-a956-4d2e-8201-4db60e7c62bb",
   "metadata": {},
   "source": [
    "self.W_q.weight.shape: torch.Size([100, 100]) <br>\n",
    "self.W_k.weight.shape: torch.Size([100, 100]) <br>\n",
    "self.W_v.weight.shape: torch.Size([100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f12636-c5c2-4a6e-998a-ba21a14b8118",
   "metadata": {},
   "source": [
    "Example.\n",
    "1. nn.LazyLinear infers the dimension of $W_k$ should be 100 by 100\n",
    "2. self.W_k(keys). ```keys.shape: torch.Size([2, 6, 100])```. The layer ```self.W_k``` is applied on all the 2*6=12 vectors to generate another 12 100D vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb58ee4-1485-43b5-bd27-80b7df7025a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "dl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
