{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09668f75-6b68-42b0-b06c-925897284547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6252e1-ee78-48b3-a9c5-545eeb81b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTFraEng(d2l.DataModule):\n",
    "    \"\"\"The English-French dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):\n",
    "        super(MTFraEng, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(\n",
    "            self._download())\n",
    "\n",
    "    def _download(self):\n",
    "        d2l.extract(d2l.download(\n",
    "            d2l.DATA_URL + 'fra-eng.zip', self.root, '94646ad1522d915e7b0f9296181140edcf86a4f5'))\n",
    "        with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        # Replace non-breaking space with space\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "        # Insert space between words and punctuation marks\n",
    "        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '\n",
    "        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "               for i, char in enumerate(text.lower())]\n",
    "        return ''.join(out)\n",
    "\n",
    "    def _tokenize(self, text, max_examples=None):\n",
    "        src, tgt = [], []\n",
    "        for i, line in enumerate(text.split('\\n')):\n",
    "            if max_examples and i > max_examples: break\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                # Skip empty tokens\n",
    "                src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])\n",
    "                tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])\n",
    "        return src, tgt\n",
    "\n",
    "    def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):\n",
    "        def _build_array(sentences, vocab, is_tgt=False):\n",
    "            pad_or_trim = lambda seq, t: (\n",
    "                seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "            if is_tgt:\n",
    "                sentences = [['<bos>'] + s for s in sentences]\n",
    "            if vocab is None:\n",
    "                vocab = d2l.Vocab(sentences, min_freq=2)  # discard the tokens with freq < 2\n",
    "            array = torch.tensor([vocab[s] for s in sentences])\n",
    "            valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "            return array, vocab, valid_len\n",
    "\n",
    "        src, tgt = self._tokenize(self._preprocess(raw_text),\n",
    "                                  self.num_train + self.num_val)\n",
    "        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)\n",
    "        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)\n",
    "        return ((src_array, tgt_array[:, :-1], src_valid_len, tgt_array[:, 1:]),\n",
    "                src_vocab, tgt_vocab)\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        idx = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader(self.arrays, train, idx)\n",
    "\n",
    "    def build(self, src_sentences, tgt_sentences):\n",
    "        raw_text = '\\n'.join([src + '\\t' + tgt for src, tgt in zip(\n",
    "            src_sentences, tgt_sentences)])\n",
    "        arrays, _, _ = self._build_arrays(\n",
    "            raw_text, self.src_vocab, self.tgt_vocab)\n",
    "        return arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68787f5-fcc6-4cec-90e9-6c5e82c2d4a2",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3344b7e-8c6c-4eb2-b79a-909cbcbe90b5",
   "metadata": {},
   "source": [
    "1. download the data\n",
    "2. preprocess: (a) Replace non-breaking space with space; (b) Insert space between words and punctuation marks\n",
    "3. tokenization and generate source and target sequences. <span style=\"color:red\">Here</span> we keep the punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5222a438-a6ea-44b0-a8f0-89f1390cecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_text[:75]: Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "Run!\tCours !\n",
      "Run!\tCourez !\n",
      "Who?\tQui ?\n",
      "Wow!\tÇa alors !\n",
      "\n",
      "text[:80]: go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça alors !\n",
      "src[:6]: [['go', '.', '<eos>'], ['hi', '.', '<eos>'], ['run', '!', '<eos>'], ['run', '!', '<eos>'], ['who', '?', '<eos>'], ['wow', '!', '<eos>']]\n",
      "tgt[:6]: [['va', '!', '<eos>'], ['salut', '!', '<eos>'], ['cours', '!', '<eos>'], ['courez', '!', '<eos>'], ['qui', '?', '<eos>'], ['ça', 'alors', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size = 10\n",
    "num_steps = 9\n",
    "data = MTFraEng(batch_size, num_steps)\n",
    "raw_text = data._download()\n",
    "print(f'raw_text[:75]: {raw_text[:75]}')\n",
    "\n",
    "text = data._preprocess(raw_text)\n",
    "print(f'text[:80]: {text[:80]}')\n",
    "\n",
    "src, tgt = data._tokenize(text)\n",
    "print(f'src[:6]: {src[:6]}')\n",
    "print(f'tgt[:6]: {tgt[:6]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a8018-f4f6-4ba8-a69f-2d157013bc39",
   "metadata": {},
   "source": [
    "### _build_arrays\n",
    "```\n",
    "def _build_array(sentences, vocab, is_tgt=False):\n",
    "    pad_or_trim = lambda seq, t: (\n",
    "        seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))\n",
    "    sentences = [pad_or_trim(s, self.num_steps) for s in sentences]\n",
    "    if is_tgt:\n",
    "        sentences = [['<bos>'] + s for s in sentences]\n",
    "    if vocab is None:\n",
    "        vocab = d2l.Vocab(sentences, min_freq=2)\n",
    "    array = torch.tensor([vocab[s] for s in sentences])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, vocab, valid_len\n",
    "```\n",
    "1. pad_or_trim: To control sentence length, we truncate long sentences and pad short ones.\n",
    "2. ```vocab = d2l.Vocab(sentences, min_freq=2)``` Discard the tokens with freq < 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc5f858-51c3-4426-a237-1eb808b354b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.src_vocab.token_to_idx: {'!': 0, ',': 1, '.': 2, '<eos>': 3, '<pad>': 4, '<unk>': 5, '?': 6, 'a': 7, 'agree': 8, 'ahead': 9, 'am': 10, 'ask': 11, 'attack': 12, 'away': 13, 'back': 14, 'bark': 15, 'be': 16, 'beats': 17, 'bed': 18, 'beg': 19, 'busy': 20, 'call': 21, 'calm': 22, 'came': 23, 'can': 24, 'catch': 25, 'cheers': 26, 'cold': 27, 'come': 28, 'cool': 29, 'cringed': 30, 'deaf': 31, 'did': 32, 'die': 33, 'died': 34, 'dogs': 35, \"don't\": 36, 'down': 37, 'dozed': 38, 'drive': 39, 'drop': 40, 'excuse': 41, 'fair': 42, 'fat': 43, 'feel': 44, 'fell': 45, 'find': 46, 'fine': 47, 'fire': 48, 'fix': 49, 'follow': 50, 'for': 51, 'forget': 52, 'free': 53, 'full': 54, 'fun': 55, 'game': 56, 'get': 57, 'give': 58, 'go': 59, 'good': 60, 'got': 61, 'grab': 62, 'had': 63, 'hang': 64, 'have': 65, 'he': 66, \"he's\": 67, 'hello': 68, 'help': 69, 'here': 70, 'hi': 71, 'him': 72, 'his': 73, 'hit': 74, 'hold': 75, 'home': 76, 'hop': 77, 'hot': 78, 'how': 79, \"how's\": 80, 'hug': 81, 'hurried': 82, 'hurry': 83, 'i': 84, \"i'll\": 85, \"i'm\": 86, \"i've\": 87, 'ill': 88, 'in': 89, 'is': 90, 'it': 91, \"it's\": 92, 'job': 93, 'join': 94, 'keep': 95, 'kiss': 96, 'knew': 97, 'know': 98, 'late': 99, 'lazy': 100, 'leave': 101, 'left': 102, \"let's\": 103, 'look': 104, 'lost': 105, 'love': 106, 'luck': 107, 'man': 108, 'marry': 109, 'may': 110, 'me': 111, 'mean': 112, 'must': 113, 'need': 114, 'new': 115, 'nice': 116, 'no': 117, 'now': 118, 'off': 119, 'ok': 120, 'okay': 121, 'on': 122, 'open': 123, 'out': 124, 'over': 125, 'paid': 126, 'phoned': 127, 'quit': 128, 'ready': 129, 'really': 130, 'refuse': 131, 'relaxed': 132, 'rested': 133, 'rich': 134, 'run': 135, 'runs': 136, 'sad': 137, 'save': 138, 'saw': 139, 'seated': 140, 'see': 141, 'she': 142, 'show': 143, 'shut': 144, 'sick': 145, 'sit': 146, 'slow': 147, 'so': 148, 'some': 149, 'soon': 150, 'speak': 151, 'stayed': 152, 'still': 153, 'stood': 154, 'stop': 155, 'sure': 156, 'swore': 157, 'take': 158, 'taste': 159, 'tell': 160, 'terrific': 161, 'that': 162, 'them': 163, 'they': 164, 'this': 165, 'tidy': 166, 'to': 167, 'tom': 168, 'too': 169, 'tried': 170, 'trust': 171, 'try': 172, 'ugly': 173, 'up': 174, 'us': 175, 'use': 176, 'wait': 177, 'wake': 178, 'warn': 179, 'wash': 180, 'watch': 181, 'way': 182, 'we': 183, 'weak': 184, 'well': 185, 'wet': 186, 'what': 187, 'who': 188, 'win': 189, 'won': 190, 'works': 191, 'write': 192, 'you': 193}\n",
      "data.tgt_vocab.token_to_idx: {'!': 0, ',': 1, '.': 2, '<bos>': 3, '<eos>': 4, '<pad>': 5, '<unk>': 6, '?': 7, 'a': 8, 'aboient': 9, 'ai': 10, 'aide': 11, 'aide-moi': 12, 'aille': 13, 'aller': 14, 'allez': 15, 'allons-y': 16, 'appelle': 17, 'asseyez-vous': 18, 'assieds-toi': 19, 'attendez': 20, 'attends': 21, 'attrape': 22, 'attrapez': 23, 'au': 24, 'aucune': 25, 'avoir': 26, 'avons': 27, 'battues': 28, 'battus': 29, 'bien': 30, 'bientôt': 31, 'bizarre': 32, 'bon': 33, 'bonne': 34, 'boulot': 35, 'bras': 36, \"c'est\": 37, 'calme': 38, 'calmes': 39, 'calmez-vous': 40, 'capté': 41, 'ceci': 42, 'certain': 43, 'chance': 44, 'chaud': 45, 'chercher': 46, 'chez': 47, 'chiens': 48, 'comme': 49, 'comment': 50, 'compris': 51, 'confiance': 52, 'continuez': 53, 'courez': 54, 'cours': 55, 'court': 56, 'dans': 57, 'de': 58, 'debout': 59, 'demande': 60, 'dois': 61, 'doucement': 62, 'du': 63, 'défaites': 64, 'défaits': 65, 'dégage': 66, 'détendu': 67, 'détendue': 68, 'elle': 69, 'elles': 70, 'emploi': 71, 'emporté': 72, 'en': 73, 'entre': 74, 'entrez': 75, 'essaye': 76, 'est': 77, 'faible': 78, 'fainéant': 79, 'fainéante': 80, 'faire': 81, 'fais': 82, 'fais-moi': 83, 'fait': 84, 'fantastique': 85, 'faut': 86, 'ferme-la': 87, 'feu': 88, 'fort': 89, 'foutre': 90, 'froid': 91, 'fûmes': 92, 'gagnèrent': 93, 'gagné': 94, 'gentil': 95, 'gentille': 96, 'gras': 97, 'gros': 98, 'homme': 99, 'hors': 100, 'ici': 101, 'il': 102, 'ils': 103, \"j'adore\": 104, \"j'ai\": 105, \"j'en\": 106, \"j'y\": 107, 'je': 108, 'joignez-vous': 109, 'juste': 110, 'j’ai': 111, \"l'ai\": 112, 'la': 113, 'laisse': 114, 'laissez': 115, 'le': 116, 'les': 117, 'libre': 118, 'lit': 119, 'lâche-toi': 120, 'l’ai': 121, \"m'en\": 122, 'ma': 123, 'maintenant': 124, 'maison': 125, 'malade': 126, 'me': 127, 'merci': 128, 'moi': 129, 'mort': 130, 'mouillé': 131, 'mouvement': 132, 'ne': 133, 'non': 134, 'nous': 135, 'occupé': 136, 'ont': 137, 'oublie': 138, 'oublie-le': 139, 'paresseuse': 140, 'paresseux': 141, 'pars': 142, 'parti': 143, 'partie': 144, 'partir': 145, 'pas': 146, 'payé': 147, 'perdu': 148, 'peu': 149, 'pigé': 150, 'plus': 151, 'porte': 152, 'poursuis': 153, 'pouvons-nous': 154, 'prie': 155, 'puis-je': 156, \"qu'est-ce\": 157, \"qu'on\": 158, 'que': 159, 'question': 160, 'qui': 161, 'quoi': 162, 'recul': 163, 'reculez': 164, 'recule\\u2009': 165, 'refuse': 166, 'rentre': 167, 'rentrez': 168, 'retard': 169, 'riche': 170, 'rouler': 171, 'réveille-toi': 172, 'réveillez-vous': 173, \"s'est\": 174, 'sais': 175, 'salut': 176, 'santé': 177, 'signe': 178, 'sois': 179, 'sors': 180, 'sortez': 181, 'soyez': 182, 'suis': 183, 'suis-je': 184, 'sérieux': 185, 'sûr': 186, 'tard': 187, 'te': 188, 'tenez': 189, 'tes': 190, 'tiens': 191, 'tom': 192, 'tomber': 193, 'tombé': 194, 'touche': 195, 'triste': 196, 'trouve': 197, 'trouvez': 198, 'tu': 199, 'un': 200, 'va': 201, 'vais': 202, 'vas-y': 203, 'venez': 204, 'venu': 205, 'viens': 206, 'vous': 207, 'vu': 208, 'y': 209, 'à': 210, 'ça': 211, 'équitable': 212, 'été': 213}\n"
     ]
    }
   ],
   "source": [
    "src, tgt, src_valid_len, label = next(iter(data.train_dataloader()))\n",
    "\n",
    "print(f'data.src_vocab.token_to_idx: {data.src_vocab.token_to_idx}')\n",
    "print(f'data.tgt_vocab.token_to_idx: {data.tgt_vocab.token_to_idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e87767-562d-4d01-8e55-e7760cd1a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: tensor([[ 86,  20,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [ 84,  32, 120,   2,   3,   4,   4,   4,   4],\n",
      "        [ 86,   5,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [ 28, 150,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [ 16, 153,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [ 84, 172,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [  5,   2,   3,   4,   4,   4,   4,   4,   4],\n",
      "        [ 84, 170,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [ 35,  15,   2,   3,   4,   4,   4,   4,   4],\n",
      "        [ 86, 121,   2,   3,   4,   4,   4,   4,   4]], dtype=torch.int32)\n",
      "tgt: tensor([[  3, 108, 183,   6,   2,   4,   5,   5,   5],\n",
      "        [  3, 108, 122, 183,  30,   6,   2,   4,   5],\n",
      "        [  3, 108, 183,   6,   2,   4,   5,   5,   5],\n",
      "        [  3, 204,  31,   0,   4,   5,   5,   5,   5],\n",
      "        [  3, 182,  39,   0,   4,   5,   5,   5,   5],\n",
      "        [  3,   6,   2,   4,   5,   5,   5,   5,   5],\n",
      "        [  3, 128,   0,   4,   5,   5,   5,   5,   5],\n",
      "        [  3,   6,   2,   4,   5,   5,   5,   5,   5],\n",
      "        [  3,   6,  48,   9,   2,   4,   5,   5,   5],\n",
      "        [  3, 108, 127, 152,  30,   2,   4,   5,   5]], dtype=torch.int32)\n",
      "source[0]: [\"i'm\", 'busy', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "target[0]: ['<bos>', 'je', 'suis', '<unk>', '.', '<eos>', '<pad>', '<pad>', '<pad>']\n",
      "source len excluding pad: tensor([4, 5, 4, 4, 4, 4, 3, 4, 4, 4], dtype=torch.int32)\n",
      "label: tensor([[108, 183,   6,   2,   4,   5,   5,   5,   5],\n",
      "        [108, 122, 183,  30,   6,   2,   4,   5,   5],\n",
      "        [108, 183,   6,   2,   4,   5,   5,   5,   5],\n",
      "        [204,  31,   0,   4,   5,   5,   5,   5,   5],\n",
      "        [182,  39,   0,   4,   5,   5,   5,   5,   5],\n",
      "        [  6,   2,   4,   5,   5,   5,   5,   5,   5],\n",
      "        [128,   0,   4,   5,   5,   5,   5,   5,   5],\n",
      "        [  6,   2,   4,   5,   5,   5,   5,   5,   5],\n",
      "        [  6,  48,   9,   2,   4,   5,   5,   5,   5],\n",
      "        [108, 127, 152,  30,   2,   4,   5,   5,   5]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print('source:', src.type(torch.int32))\n",
    "print('tgt:', tgt.type(torch.int32))\n",
    "print('source[0]:', data.src_vocab.to_tokens(src[0].type(torch.int32)))\n",
    "print('target[0]:', data.tgt_vocab.to_tokens(tgt[0].type(torch.int32)))\n",
    "print('source len excluding pad:', src_valid_len.type(torch.int32))\n",
    "print('label:', label.type(torch.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35e539-fce7-46fc-a541-9b08d001d70b",
   "metadata": {},
   "source": [
    "### The Encoder-Decoder Seq2Seq Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9dd67-bb2d-40fe-9250-004a9488a212",
   "metadata": {},
   "source": [
    "[Sequence to Sequence Learning with Neural Networks, 2014](https://arxiv.org/pdf/1409.3215.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2a9ca-b6df-4962-813e-a0fea3145411",
   "metadata": {},
   "source": [
    "input -> encoder -> state -> decoder (<- input) -> output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3231eac3-65ff-4dd0-bad6-b4d565fa22a0",
   "metadata": {},
   "source": [
    "1. **encoder**: enc_input -> enc_outputs\n",
    "2. **decoder**: dec_input $\\times$ dec_state -> dec_outputs <br>\n",
    "   **dec_init_state**: enc_outputs -> dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe45645-5226-47ac-bd5a-84c85caa6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"The base encoder interface for the encoder-decoder architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding)\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23875262-3be4-4411-b3de-6b19444597c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"The base decoder interface for the encoder-decoder architecture.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Later there can be additional arguments (e.g., length excluding padding)\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb9725c3-8af3-4fce-914c-f81a83a79317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(d2l.Classifier):\n",
    "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_all_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_all_outputs, *args)\n",
    "        # Return decoder output only\n",
    "        return self.decoder(dec_X, dec_state)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599d51f0-54c5-400a-be26-0476b0138458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seq2seq(module):\n",
    "    \"\"\"Initialize weights for Seq2Seq.\"\"\"\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.GRU:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e7bc3d1-e777-48c7-a624-adfb5a2423fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        embs = self.embedding(X.t().type(torch.int64))\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        outputs, state = self.rnn(embs)\n",
    "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4b7210-1cbb-4f8b-81e2-02a0a6a52eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = d2l.GRU(embed_size + num_hiddens, num_hiddens,\n",
    "                           num_layers, dropout)\n",
    "        self.dense = nn.LazyLinear(vocab_size)\n",
    "        self.apply(init_seq2seq)\n",
    "\n",
    "    def init_state(self, enc_all_outputs, *args):\n",
    "        \"\"\"\n",
    "        :param enc_all_outputs: (outputs, state)\n",
    "        \"\"\"\n",
    "        return enc_all_outputs\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # X shape: (batch_size, num_steps)\n",
    "        # embs shape: (num_steps, batch_size, embed_size)\n",
    "        embs = self.embedding(X.t().type(torch.int32))\n",
    "        enc_output, hidden_state = state\n",
    "        # context shape: (batch_size, num_hiddens)\n",
    "        context = enc_output[-1]  # enc_output at the final time step\n",
    "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
    "        context = context.repeat(embs.shape[0], 1, 1)\n",
    "        # Concat at the feature dimension\n",
    "        embs_and_context = torch.cat((embs, context), -1)\n",
    "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
    "        outputs = self.dense(outputs).swapaxes(0, 1)\n",
    "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
    "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
    "        return outputs, [enc_output, hidden_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcba2b-8067-4bc3-bb5c-d17417351b58",
   "metadata": {},
   "source": [
    "### init_seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a9b4d-2113-4a62-b5aa-5aa9f7077732",
   "metadata": {},
   "source": [
    "1. nn.linear: in_features $\\times$ out_features -> (\\*, out_features) <br>\n",
    "   input: (*, in_features) <br>\n",
    "   output: (\\*, out_features) <br>\n",
    "   $y = x A^{T} + b$\n",
    "2. by default, bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe3e255a-4404-4416-91c7-bcfd0b8f9513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape: torch.Size([10, 3])\n",
      "m.weight.shape, m.bias.shape: (torch.Size([3, 2]), torch.Size([3]))\n"
     ]
    }
   ],
   "source": [
    "in_feas, out_feas, num_input = 2, 3, 10\n",
    "m = nn.Linear(in_feas, out_feas)\n",
    "input = torch.randn(num_input, in_feas)\n",
    "output = m(input)\n",
    "print(f'output.shape: {output.shape}')\n",
    "print(f'm.weight.shape, m.bias.shape: {m.weight.shape, m.bias.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15b48e-9989-4b43-a205-f5fd6eab945c",
   "metadata": {},
   "source": [
    "### nn.Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493a187-9930-4d53-97b5-ebf337828c27",
   "metadata": {},
   "source": [
    "1. nn.Embedding: num_embeddings $\\times$ embedding_dim, OR vocab_size $\\times$ vector_size\n",
    "2. num_embeddings: size of the dictionary of embeddings\n",
    "3. embedding_dim: the size of each embedding vector\n",
    "4. input: (*), IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "5. output: (*, H), where * is the input shape and H = embedding_dim\n",
    "6. vocab_size > any element in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4529df3-370f-42f1-8d4e-9db8ca16b06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2203,  1.3139,  1.0533],\n",
       "         [ 0.1388, -0.2044, -0.8036],\n",
       "         [-0.7979,  0.1838,  1.6863],\n",
       "         [ 1.6221, -1.4779,  1.1331]],\n",
       "\n",
       "        [[-0.7979,  0.1838,  1.6863],\n",
       "         [-0.2808,  0.7697, -0.6596],\n",
       "         [ 0.1388, -0.2044, -0.8036],\n",
       "         [-0.5951, -0.7112,  0.6230]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emb_size = 10, 3\n",
    "embedding = nn.Embedding(vocab_size, emb_size)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1, 2, 4, 0], [4, 3, 2, 9]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbda45-f2e9-4eaa-bad3-f26751cc71c0",
   "metadata": {},
   "source": [
    "Let us take the 1st batch in the input ```[1, 2, 4, 0]``` as an example. Its embedding matrix (4 by 3) is a re-representation of the raw data.nts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d50477-e85e-4dde-b7b7-b032ae3d776d",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f0b27c9-5cda-49ee-a570-dfc2797aae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_outputs.shape: torch.Size([9, 4, 16])\n",
      "enc_state.shape: torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
    "batch_size, num_steps = 4, 9\n",
    "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "X = torch.zeros((batch_size, num_steps))\n",
    "enc_outputs, enc_state = encoder(X)\n",
    "print(f'enc_outputs.shape: {enc_outputs.shape}')\n",
    "print(f'enc_state.shape: {enc_state.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514bb06-7346-45e8-bce1-bdc6951eb995",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2c903e-97c4-4c80-a74c-eccf9cae1b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\billt\\PycharmProjects\\DeepLearning\\dl_venv\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
    "state = decoder.init_state(encoder(X))\n",
    "dec_outputs, state = decoder(X, state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "dl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
